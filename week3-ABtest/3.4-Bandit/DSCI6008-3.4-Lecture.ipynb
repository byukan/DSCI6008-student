{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3.4: Multi-Arm Bandit  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Scenario  \n",
    "\n",
    "Consider the following scenario:  \n",
    "\n",
    "\n",
    "* The company you work for is testing out a new version of it's website.\n",
    "* After running an A/B test for an afternoon, the new version of the site appears to performing slightly better than the old version.\n",
    "* After running the test over night, the new version of the site is performing better than the old version with a statistically significant p-value of 0.04.\n",
    "\n",
    "\n",
    "Do you stop the test, or do you keep running it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration vs. Exploitation  \n",
    "\n",
    "When we are trying to determine which option, from a potential\n",
    "pool, is the best option what we are faced with is an information\n",
    "gathering challenge.  \n",
    "\n",
    "* **Exploration**: Trying out different options to try and determine\n",
    "the reward associated with the given approach (i.e. acquiring\n",
    "more knowledge)  \n",
    "* **Exploitation**: Going with the approach that you believe to\n",
    "have the highest expected payoff (i.e. optimizing decisions\n",
    "based on existing knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional A/B Testing  \n",
    "\n",
    "* A short period of *pure exploration*, in which you assign\n",
    "equal numbers of users to Group A and Group B  \n",
    "* A long period of *pure exploitation*, in which you send\n",
    "all of your users to the more successful version of your\n",
    "site and never come back to the option that seemed to\n",
    "be inferior  \n",
    "\n",
    "**Q**: Why might this be a bad strategy?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Potential Inefficiencies  \n",
    "\n",
    "* Equal number of observations are routed to A and B for a\n",
    "preset amount of time or iterations  \n",
    "* Need to wait for the experiment to conclude for certain\n",
    "statistical guarantees to be provided  \n",
    "* Only after that preset amount of time or iterations do we stop\n",
    "and use the better performer  \n",
    "* This wastes time - and money - showing users the\n",
    "suboptimal site  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Arm Bandit Approach \n",
    "\n",
    "* Shows a user the site that you think is best most of the time\n",
    "(exactly how is dictated by the strategy chosen)\n",
    "* As the experiment runs, we update the belief about the true\n",
    "CTR (Click Through Rate)\n",
    "* Run for however long until we are satisfied the experiment has\n",
    "determined the better site\n",
    "* Balances exploration and exploitation rather than doing only\n",
    "one or the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Is a Multi-Armed Bandit?  \n",
    "\n",
    "* Each slot machine (a.k.a. one-armed bandit) has a different (unknown!)\n",
    "chance of winning.\n",
    "* How do you maximize your total payout after a finite number of plays?\n",
    "* Assume all have the same payoff. (“binary bandits”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**:  \n",
    "* which version of an web ad has a higher click-through rate\n",
    "(CTR)?\n",
    "* which website is easier to navigate?\n",
    "* which drug is more effective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cases  \n",
    "\n",
    "* Dynamic A/B Testing\n",
    "* Budget allocation amongst competing projects\n",
    "* Clinical trials\n",
    "* Adaptive routing in attempts to minimize network delays\n",
    "* Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing Regret  \n",
    "\n",
    "* Regret is the difference of what we won and what we would expect with optimal strategy\n",
    "\n",
    "$$ \n",
    "\\begin{align*} \n",
    "\\text{regret} &= \\sum_{i = 1}^k (p_{opt} - p_i)   \\\\ \n",
    "&= k p_{opt} - \\sum_{i = 1}^k p_i\n",
    "\\end{align*} $$\n",
    "\n",
    "* where there are k trials and $p_i$ is the probability of winning with the bandit chosen on the i-th run. $p_{opt}$ is the probability of winning with the best bandit\n",
    "\n",
    "* We can view this as a cost function we are trying to minimize  \n",
    "\n",
    "\n",
    "* A regret of 0 would mean you always played the best machine - this isn't ever possible since you need to collect data to determine which one is the best\n",
    "\n",
    "* Note that you need to know the true probabilities to calculate the regret. This is a theoretical idea to evaluate which algorithm is best and is probably not something you would calculate in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-Greedy Algorithm  \n",
    "\n",
    "* In the epsilon-greedy algorithm, some percent of the time we explore and randomly choose one of the options. The rest of the time we choose the option that has so far had the highest conversion rate. epsilon is a value between 0 and 1 which is the probability that we explore. 10% is a standard choice for epsilon.  \n",
    "\n",
    "\n",
    "* Here's the pseudocode:  \n",
    "\n",
    "```\n",
    "randomly choose a number r from 0 to 1\n",
    "if r < epsilon:\n",
    "    randomly choose one of the possible sites, each with equal probability\n",
    "else:\n",
    "    choose the site with the highest conversion rate from the data so far\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each version will have seen a different number of users. You will calculate each conversion rate like this:  \n",
    "\n",
    "\n",
    "```\n",
    "                             # clicks from site A\n",
    "conversion rate of site A = ----------------------\n",
    "                              # visits of site A\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This algorithm is simple, but it has some flaws.\n",
    "\n",
    "    * At the beginning, we are trying to do exploitation, but we don't yet know which version is right.\n",
    "\n",
    "    * If epsilon is 90%, the epsilon-greedy algorithm will eventually show the best option 90% of the time. So this means that 10% of the time the algorithm will continue to randomly show different versions of the site, no matter how confident we are that a certain version is the best. There is a point where we should stop the exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax  \n",
    "\n",
    "* In this algorithm, we choose each version of the site in proportion to its estimated value using a Boltzman Distribution. Say $p_A$, $p_B$ and $p_C$ are the conversion rates for three versions of the site. We would choose site A with the following probability  \n",
    "\n",
    "$$ \\frac{exp(p_A/ \\tau)}{exp(p_A/ \\tau) + exp(p_B/ \\tau) + exp(p_C/ \\tau)} $$\n",
    "\n",
    "* $\\tau$ is a choosen parameter that controls the ‘randomness’ of the\n",
    "choice, usually around 0.001  \n",
    "\n",
    "\n",
    "* It doesn’t take into account how many times it’s pulled an arm\n",
    "* A member of probability matching algorithms, so-called for their property of creating the probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper Confidence Bound (UCB)\n",
    "\n",
    "* This is a class of algorithms that can be proven to have a logarithmic upper bound for regret. We won't worry about the mathematical proof and will just be focusing on the UCB1 algorithm  \n",
    "\n",
    "* We choose the strategy that where the following value is the largest  \n",
    "\n",
    "$$ p_A + \\sqrt{\\frac{2 log(N)}{n_A}} $$  \n",
    "\n",
    "* where $N$ is the total number of rounds (for all sites) and $n_A$ is the number of times site A has been shown. $p_A$ is the conversion rate for that version so far  \n",
    "\n",
    "* You should first make sure to play each bandit once so none of the $n_A$'s are 0  \n",
    "\n",
    "\n",
    "* UCB doesn’t use randomness at all. Unlike epsilon-greedy, it’s possible to know exactly how UCB will\n",
    "behave in any given situation. This can make it easier to\n",
    "reason about at times  \n",
    "\n",
    "* UCB doesn’t have free parameters that you need to\n",
    "configure before you can deploy it. This is a major\n",
    "improvement if you’re interested in running in the wild,\n",
    "because it means that you can start UCB without having\n",
    "clear sense of how you expect the world to behave "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Bandit  \n",
    "\n",
    "* We can use the Bayesian Beta-Binomial conjugate prior\n",
    "techniques used to model the click-through rate as our base model for each of the bandits  \n",
    "* This is another probability matching algorithm where we have a separate model for each of the bandits  \n",
    "\n",
    "\n",
    "* Each bandit has an associated beta distribution with parameters:  \n",
    "\n",
    "    * $\\alpha = 1 + $ number of times bandit has won\n",
    "    * $\\beta = 1 + $ number of times bandit has lost  \n",
    "  \n",
    "* Sample from each bandit’s distribution and play the bandit with the\n",
    "highest value  \n",
    "\n",
    "\n",
    "* It will naturally converge on the bandit with the best payout  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Assume that the click-through rates of versions A and B of a website are $p_A = 0.53$ and $p_B = 0.67$. Simulate running the multi-arm bandit using these strategies. Plot (1) the percentage of time the optimal bandit was chosen over time, (2) the total regret over time for each algorithm.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
